{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do some GIS using Geopandas: \n",
    "#### An introduction for novices, by a novice. \n",
    "***Purpose:*** To showcase a clean Jupyter notebook that outputs webmap-ready data (GeoJSON).  This notebook will incorporate Geopandas, and a variety of other Python libraries to import, wrangle, analyze, and export data within a typical data-viz and mapping workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Created by Ritchie Katko (rakatk0@uky.edu) for [UKy Geography's New Maps Plus](http://newmapsplus.uky.edu/) MAP674 Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objective: \n",
    "This notebook will fulfill its purpose through the exploration of New York City Police Department's [\"Stop, Question, Frisk\" Data](https://www1.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page) for the year 2016.  This data originates as a CSV, projected in [EPSG:2263 NAD83 / New York Long Island (ftUS)](https://epsg.io/2263). It will have to be cleaned up and reprojected to be ready for analysis against other geospatial data (see below).\n",
    "\n",
    "Please note the following: The data wrangling and analysis performed in this notebook is intended to function as a geopandas tutorial, rather than as a piece of data-journalism. \n",
    "\n",
    "Data utilized in this notebook: \n",
    "- [NYPD Precinct Boundaries](https://data.cityofnewyork.us/Public-Safety/Police-Precincts/78dh-3ptz)\n",
    "- [\"Stop, Question, Frisk\" Data](https://www1.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the environment\n",
    "Here's my background specs: \n",
    "- ***OS***: Ubuntu 16.04.6 LTS\n",
    "- ***Python version***: `$ /usr/bin/python3 -V` 3.5.2\n",
    "- ***Anaconda3 version***: `$ conda info` 4.5.11\n",
    "\n",
    "###### Conda Environment \n",
    "The following packages must be installed (`$ conda install`) into the active Conda environment:\n",
    "- `Jupyter` (allows use of jupyter coding environment\n",
    "- `Geopandas` (integrates other packages geospatial components) \n",
    "- `Matplotlib` (dataviz package - allows plotting of visualizations)\n",
    "\n",
    "##### after activating the Conda Environment of choice, navigate to the working directory and run `jupyter notebook` to initiate the browser-based notebook environment.  \n",
    "\n",
    "##### Now, let's get to work: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we'll Add packages to allow for their use in this environment\n",
    "From the [pandas website](https://pandas.pydata.org/): \"pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language\"\n",
    "\n",
    "From the [numpy website](https://numpy.org/): \"Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages, using aliases to simplify code\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Local Data\n",
    "We'll load a CSV into the notebook as dataframe `df`. This CSV is hosted locally within the repository. Using the `.type()` method, the cell will output the type of data.  In this case it is a `pandas.core.frame.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv into notebook as dataframe 'df', and check the data  \n",
    "path_to_file = './data/sqf-2016.csv'\n",
    "# url version: path_to_file = 'https://www1.nyc.gov/assets/nypd/downloads/excel/analysis_and_planning/stop-question-frisk/sqf-2016.csv'\n",
    "\n",
    "df = pd.read_csv(path_to_file,  dtype = str) # load csv data as pandas DataFrame, load # dtype = str, \n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take a look at the data, from a few angles\n",
    "Using `.shape` provides a simple look at the number of rows and columns.  using `.sample(X)` provides a random sampling of X # of rows for inspection.   `.head()` and `.tail()` return the first 5, and last 5 rows, respectively. Using `list(df.columns)` will return a list of all the columns within the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12405, 112),\n",
       "       year  pct ser_num datestop timestop recstat inout trhsloc perobs  \\\n",
       " 9255  2016   81     107  8292016     2350       1     O       P   1.00   \n",
       " 6609  2016  103      57  5262016     1550       1     O       P   1.00   \n",
       " \n",
       "         crimsusp  ... zip addrpct sector beat post   xcoord   ycoord dettypCM  \\\n",
       " 9255         CPW  ...          81      B    4       1001222   189511       CM   \n",
       " 6609  CPW- KNIFE  ...         103      B    3       1039709   196693       CM   \n",
       " \n",
       "      lineCM detailCM  \n",
       " 9255      1       20  \n",
       " 6609      1       20  \n",
       " \n",
       " [2 rows x 112 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df.sample(2)  # shape will return the number of rows, and columns.  .sample() will return a random row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   year pct ser_num datestop timestop recstat inout trhsloc perobs  \\\n",
       " 0  2016  41      22  2072016      100       A     O       P   1.00   \n",
       " 1  2016  10      22  2182016       30       1     O       P   8.00   \n",
       " 2  2016  66       1  1012016       30       1     I       P   2.00   \n",
       " 3  2016  47      18  1012016       40       1     O       H   1.00   \n",
       " 4  2016  79       1  1012016       50       1     O       P   3.00   \n",
       " \n",
       "       crimsusp  ... zip addrpct sector beat post   xcoord   ycoord dettypCM  \\\n",
       " 0         BURG  ...          41      B    2       1013353   234000       CM   \n",
       " 1  MISDEMEANOR  ...          10      D             983478   212373       CM   \n",
       " 2          FEL  ...          66      F             988340   172111       CM   \n",
       " 3          FEL  ...          47      C                                   CM   \n",
       " 4       D.W.I.  ...          79      G    4        998197   187413       CM   \n",
       " \n",
       "   lineCM detailCM  \n",
       " 0      1       14  \n",
       " 1      1       28  \n",
       " 2      1        9  \n",
       " 3      1       20  \n",
       " 4      1      112  \n",
       " \n",
       " [5 rows x 112 columns],\n",
       "        year  pct ser_num  datestop timestop recstat inout trhsloc perobs  \\\n",
       " 12400  2016  123      89  12022016     1847       1     O       P   1.00   \n",
       " 12401  2016  123      90  12022016      255       1     O       P   5.00   \n",
       " 12402  2016  123      91  12042016     2225       1     O       P   2.00   \n",
       " 12403  2016  123      92  11222016     2212       1     O       P   1.00   \n",
       " 12404                                                                      \n",
       " \n",
       "       crimsusp  ... zip addrpct sector beat post   xcoord   ycoord dettypCM  \\\n",
       " 12400      FEL  ...         123      G             935054   136263       CM   \n",
       " 12401     MISD  ...         123      B             917943   127951       CM   \n",
       " 12402     MISD  ...         123      C             930949   133127       CM   \n",
       " 12403     MISD  ...         123      F             939042   132823       CM   \n",
       " 12404           ...                                                           \n",
       " \n",
       "       lineCM detailCM  \n",
       " 12400      1        9  \n",
       " 12401      1       80  \n",
       " 12402      1       68  \n",
       " 12403      1       20  \n",
       " 12404                  \n",
       " \n",
       " [5 rows x 112 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(), df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'pct',\n",
       " 'ser_num',\n",
       " 'datestop',\n",
       " 'timestop',\n",
       " 'recstat',\n",
       " 'inout',\n",
       " 'trhsloc',\n",
       " 'perobs',\n",
       " 'crimsusp',\n",
       " 'perstop',\n",
       " 'typeofid',\n",
       " 'explnstp',\n",
       " 'othpers',\n",
       " 'arstmade',\n",
       " 'arstoffn',\n",
       " 'sumissue',\n",
       " 'sumoffen',\n",
       " 'compyear',\n",
       " 'comppct',\n",
       " 'offunif',\n",
       " 'officrid',\n",
       " 'frisked',\n",
       " 'searched',\n",
       " 'contrabn',\n",
       " 'adtlrept',\n",
       " 'pistol',\n",
       " 'riflshot',\n",
       " 'asltweap',\n",
       " 'knifcuti',\n",
       " 'machgun',\n",
       " 'othrweap',\n",
       " 'pf_hands',\n",
       " 'pf_wall',\n",
       " 'pf_grnd',\n",
       " 'pf_drwep',\n",
       " 'pf_ptwep',\n",
       " 'pf_baton',\n",
       " 'pf_hcuff',\n",
       " 'pf_pepsp',\n",
       " 'pf_other',\n",
       " 'radio',\n",
       " 'ac_rept',\n",
       " 'ac_inves',\n",
       " 'rf_vcrim',\n",
       " 'rf_othsw',\n",
       " 'ac_proxm',\n",
       " 'rf_attir',\n",
       " 'cs_objcs',\n",
       " 'cs_descr',\n",
       " 'cs_casng',\n",
       " 'cs_lkout',\n",
       " 'rf_vcact',\n",
       " 'cs_cloth',\n",
       " 'cs_drgtr',\n",
       " 'ac_evasv',\n",
       " 'ac_assoc',\n",
       " 'cs_furtv',\n",
       " 'rf_rfcmp',\n",
       " 'ac_cgdir',\n",
       " 'rf_verbl',\n",
       " 'cs_vcrim',\n",
       " 'cs_bulge',\n",
       " 'cs_other',\n",
       " 'ac_incid',\n",
       " 'ac_time',\n",
       " 'rf_knowl',\n",
       " 'ac_stsnd',\n",
       " 'ac_other',\n",
       " 'sb_hdobj',\n",
       " 'sb_outln',\n",
       " 'sb_admis',\n",
       " 'sb_other',\n",
       " 'repcmd',\n",
       " 'revcmd',\n",
       " 'rf_furt',\n",
       " 'rf_bulg',\n",
       " 'offverb',\n",
       " 'offshld',\n",
       " 'forceuse',\n",
       " 'sex',\n",
       " 'race',\n",
       " 'dob',\n",
       " 'age',\n",
       " 'ht_feet',\n",
       " 'ht_inch',\n",
       " 'weight',\n",
       " 'haircolr',\n",
       " 'eyecolor',\n",
       " 'build',\n",
       " 'othfeatr',\n",
       " 'addrtyp',\n",
       " 'rescode',\n",
       " 'premtype',\n",
       " 'premname',\n",
       " 'addrnum',\n",
       " 'stname',\n",
       " 'stinter',\n",
       " 'crossst',\n",
       " 'aptnum',\n",
       " 'city',\n",
       " 'state',\n",
       " 'zip',\n",
       " 'addrpct',\n",
       " 'sector',\n",
       " 'beat',\n",
       " 'post',\n",
       " 'xcoord',\n",
       " 'ycoord',\n",
       " 'dettypCM',\n",
       " 'lineCM',\n",
       " 'detailCM']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns) # take a look at all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### are there any columns we don't care about that can be removed?   \n",
    "This dataset has granular detail on 12000+ incidents.  Let's simplify this. Taking a look at a key provided for this data, there are many detailed records for each individual stop.  The dataset provides information on if an arrest or summons was made, if a weapon was found, if force was used, demographic info about the person stopped, and location information about when and where the stop occured, as well as information regarding the officer who made the stop. Quite a few columnns, for the purpose of this investigation, can be removed.  To do so, one can use the `.drop() method`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['year',\n",
    " 'ser_num',\n",
    " 'recstat',\n",
    " 'inout',\n",
    " 'trhsloc',\n",
    " 'perobs',\n",
    " 'perstop',\n",
    " 'typeofid',\n",
    " 'explnstp',\n",
    " 'othpers',\n",
    " 'compyear',\n",
    " 'comppct',\n",
    " 'offunif',\n",
    " 'officrid',\n",
    " 'adtlrept',\n",
    " 'riflshot',\n",
    " 'asltweap',\n",
    " 'machgun',\n",
    " 'othrweap',\n",
    " 'radio',\n",
    " 'ac_rept',\n",
    " 'ac_inves',\n",
    " 'rf_vcrim',\n",
    " 'rf_othsw',\n",
    " 'ac_proxm',\n",
    " 'rf_attir',\n",
    " 'cs_objcs',\n",
    " 'cs_descr',\n",
    " 'cs_casng',\n",
    " 'cs_lkout',\n",
    " 'rf_vcact',\n",
    " 'cs_cloth',\n",
    " 'cs_drgtr',\n",
    " 'ac_evasv',\n",
    " 'ac_assoc',\n",
    " 'cs_furtv',\n",
    " 'rf_rfcmp',\n",
    " 'ac_cgdir',\n",
    " 'rf_verbl',\n",
    " 'cs_vcrim',\n",
    " 'cs_bulge',\n",
    " 'cs_other',\n",
    " 'ac_incid',\n",
    " 'ac_time',\n",
    " 'rf_knowl',\n",
    " 'ac_stsnd',\n",
    " 'ac_other',\n",
    " 'sb_hdobj',\n",
    " 'sb_outln',\n",
    " 'sb_admis',\n",
    " 'sb_other',\n",
    " 'repcmd',\n",
    " 'revcmd',\n",
    " 'rf_furt',\n",
    " 'rf_bulg',\n",
    " 'offverb',\n",
    " 'offshld',\n",
    " 'forceuse',\n",
    " 'dob',\n",
    " 'ht_feet',\n",
    " 'ht_inch',\n",
    " 'weight',\n",
    " 'haircolr',\n",
    " 'eyecolor',\n",
    " 'build',\n",
    " 'othfeatr',\n",
    " 'addrtyp',\n",
    " 'rescode',\n",
    " 'premtype',\n",
    " 'premname',\n",
    " 'addrnum',\n",
    " 'stname',\n",
    " 'stinter',\n",
    " 'crossst',\n",
    " 'aptnum',\n",
    " 'state',\n",
    " 'zip',\n",
    " 'addrpct',\n",
    " 'sector',\n",
    " 'beat',\n",
    " 'post',\n",
    " 'dettypCM',\n",
    " 'lineCM',\n",
    " 'detailCM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pct',\n",
       " 'datestop',\n",
       " 'timestop',\n",
       " 'crimsusp',\n",
       " 'arstmade',\n",
       " 'arstoffn',\n",
       " 'sumissue',\n",
       " 'sumoffen',\n",
       " 'frisked',\n",
       " 'searched',\n",
       " 'contrabn',\n",
       " 'pistol',\n",
       " 'knifcuti',\n",
       " 'pf_hands',\n",
       " 'pf_wall',\n",
       " 'pf_grnd',\n",
       " 'pf_drwep',\n",
       " 'pf_ptwep',\n",
       " 'pf_baton',\n",
       " 'pf_hcuff',\n",
       " 'pf_pepsp',\n",
       " 'pf_other',\n",
       " 'sex',\n",
       " 'race',\n",
       " 'age',\n",
       " 'city',\n",
       " 'xcoord',\n",
       " 'ycoord']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return a list of remaining columns \n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### lets create a few new columns. was the person innocent?  and, was the person carrying a pistol or knife?  and, was force used by the police? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called df.crime where the value is yes\n",
    "# if df.arstmade or df.sumissue are Y\n",
    "\n",
    "#df['crime'] = np.where((df['arstmade'] == 'Y')|(df['sumissue'] == df['Y']), 'yes', 'no')\n",
    "\n",
    "#df['weapon'] = np.where((df['pistol'] == 'Y')|(df['knifcuti'] == df['N']), 'yes', 'no')\n",
    "#df['force_used'] = np.where((df['pf_hands'] == 'Y')|\n",
    "                          #  (df['pf_wall'] == df['Y'])|\n",
    "                          #  (df['pf_grnd'] == df['Y'])|\n",
    "                          #  (df['pf_drwep'] == df['Y'])|\n",
    "                          #  (df['pf_ptwep'] == df['Y'])|\n",
    "                           # (df['pf_baton'] == df['Y'])|\n",
    "                           # (df['pepsp'] == df['Y'])|\n",
    "                           # (df['pf_other'] == df['Y']), 'yes', 'no')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### after taking a look at the data and identifying some coordinates, further inspect the geometry columns.\n",
    "\n",
    "These are labeled `xcoord` and `ycoord`.  Let's figure out what type of data this is using `type()` along with bracket notation to drill into specific cells within columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, str)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check datatype of 'xcoord' & 'ycoord'\n",
    "type(df.xcoord[1]), type(df.ycoord[1]) # str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### since these are strings, removing extra spaces at the beginning or end of the string that may be lingering is necessary before converting over to a numerical type\n",
    "This is done with using the `.map()` method (which allows for changes to an entire series), with the `.str.strip()`.  This was completed for a variety of columns that utilized strings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extra spaces in strings\n",
    "df[\"xcoord\"] = df[\"xcoord\"].map(str.strip)  \n",
    "df[\"ycoord\"] = df[\"ycoord\"].map(str.strip)\n",
    "df[\"datestop\"] = df[\"datestop\"].map(str.strip)\n",
    "df[\"timestop\"] = df[\"timestop\"].map(str.strip)\n",
    "df[\"pct\"] = df[\"pct\"].map(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1013353\n",
       " 1     983478\n",
       " 2     988340\n",
       " 3           \n",
       " 4     998197\n",
       " Name: xcoord, dtype: object, 0    234000\n",
       " 1    212373\n",
       " 2    172111\n",
       " 3          \n",
       " 4    187413\n",
       " Name: ycoord, dtype: object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the first 5 rows of data in this column\n",
    "df.xcoord.head(), df.ycoord.head()  # and we see the 4th row has no coordinate in either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding an empty row  in the first 5 of 12405 rows is _sheer luck_. If the first 5 rows were geocoded, that error would not have been observed. How could we search for these? \n",
    "\n",
    "Since these are `str`, they won't have any `'NaN'` values.  Rather than visually parse this for bad data, lets assume that any cells without coordinates have some other value within them (or are empty strings, especially since we stripped zeros).  If we sort for the most frequent value in this column, it might return a default string value when there is no location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see what the most common value\n",
    "# .idxmax() returns the most common value in a column\n",
    "df.xcoord.value_counts().idxmax(), df.ycoord.value_counts().idxmax()  # answer will an empty string value ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### after identifying the cell value that indicates ***not geocoded***,  now remove ungeocoded cells from the dataframe\n",
    "This will occur in 2 phases.  In the first,  replace all empty strings `''`, with `'NaN'`, and then check the datashape for comparison after phase 2.  In phase 2, we will drop all rows with `'NaN'` values in the columns specified, and then check the shape to verify that rows were dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12405, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace empty strings with NaN\n",
    "df['xcoord'].replace('', np.nan)\n",
    "df['ycoord'].replace('', np.nan)\n",
    "df.shape # check number of rows here for comparison with result of next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12405, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now drop the null values\n",
    "df.dropna(subset=['xcoord'])\n",
    "df.dropna(subset=['ycoord'])\n",
    "df.shape # check number of rows here for comparison with result of previous step to verify removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # to be useful later, we need numerical values (for now, integers), not Strings. \n",
    "Converting the coordinates into numerical values is paramount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert strings to integers\n",
    "df.xcoord = pd.to_numeric(df.xcoord, errors='ignore')\n",
    "df.ycoord = pd.to_numeric(df.ycoord, errors='ignore')\n",
    "type(df.xcoord[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After deleting some rows, resetting the index is important.  \n",
    "But before that happens, reordering chronologically might be useful.  First, convert strings to integers, then sort in chronological order, then reset the index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e2fb237fcf21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatestop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datestop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatestop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datestop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datestop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sort dataframe by 'date', will sort smallest to largest value, so january 1 will be first!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# convert date strings to integers  & sort chronologically\n",
    "#df.datestop.replace('', np.nan)\n",
    "#df.dropna(subset=['datestop'])\n",
    "df.datestop = df['datestop'].apply(int)\n",
    "df.sort_values(by=['datestop'],inplace = True) # sort dataframe by 'date', will sort smallest to largest value, so january 1 will be first! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df = df.reset_index()\n",
    "\n",
    "# delete prior index\n",
    "del df['index'] \n",
    "print(df.datestop.head())  #note these first 5 values are all on jan 1 2016, verifying it worked! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets drill into the dataset a bit with some queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### which precinct had the most number of stop and frisks in 2016?  Let's figure that out and end up with a dataframe for merging with our Precincts GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract series from df \n",
    "counts = df['pct'].value_counts()\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to pandas dataframe & reset index\n",
    "counts = pd.DataFrame(counts).reset_index()\n",
    "type(counts), counts.shape, counts.columns, counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns \n",
    "counts.rename(columns={'index': 'precinct', 'pct': 'counts'}, inplace=True)\n",
    "counts.head(), type(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### we'll park this here and come back when we import the NYPD Precincts geojson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now, find all instances with 'use of force'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find total number of instances of use of force used\n",
    "a = df.loc[(df['pf_pepsp'] == 'Y') | (df['pf_hands'] == 'Y') | (df['pf_wall'] == 'Y') | (df['pf_grnd'] == 'Y') | (df['pf_drwep'] == 'Y') | (df['pf_ptwep'] == 'Y') | (df['pf_baton'] == 'Y') | (df['pf_hcuff'] == 'Y') | (df['pf_other'] == 'Y')]\n",
    "count = str(a.shape[0])\n",
    "print('There were ' + count + ' uses of force during SQF in 2016.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find all instances of minors in the Bronx caught with pistols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pistol'].unique() # verify that values are present\n",
    "a = df.loc[df['pistol'].isin(['Y']) & (df['city'] == 'BRONX') & (df['age'] < '18')] # query for various conditions\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stash the data locally as json\n",
    "with open('./data/2016stops.json', 'w') as f:\n",
    "    f.write(df.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now verify the file was written "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports the OS module, using the .walk() method returns information about the current working directory. \n",
    "import os\n",
    "for root, dirs, files in os.walk('./data/'):\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so, we've got a JSON.  To continue, we'll need to define geometry for this using Shapely, prior to converting into a GeoDataFrame for all our awesome web-mapping applications downstream.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing geopandas and shapely modules will allow convert coordinates into points with a projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### access JSON that we created and stashed locally and load as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = './data/2016stops.json'\n",
    "df = pd.read_json(path_to_file) # load csv data as pandas DataFrame\n",
    "type(df), df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create the geometry...\n",
    "this requires us to know the projection that the `xcoord` and `ycoord` were presented.  after a bit of digging, I was able to determine that NYPD used ESPG:2263 as the projected coordinate system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geometry using Shapely Point & assign proper projection\n",
    "geometry = [Point(xy) for xy in zip(df.xcoord, df.ycoord)]\n",
    "df = df.drop(['xcoord', 'ycoord'], axis=1)\n",
    "crs = {'init': 'epsg:2263'}\n",
    "gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reproject data to web-map ready EPSG:4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_crs(epsg=4326, inplace=True)\n",
    "#gdf.to_crs({'init': 'epsg:4269'})\n",
    "gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visualize the data (are we mapping yet?), but first we'll need to import some plotting tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotter \n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# change default figsize\n",
    "plt.rcParams['figure.figsize'] = (30, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    " \n",
    "base_color = '#f0f0f0'\n",
    "border_color = base_color\n",
    " \n",
    "gdf.plot(ax=ax, color='red', alpha=0.1, zorder=0, label='stops');  #low opacity adds a heatmap effect at this scale.  unintentional\n",
    " \n",
    "ax.set(title=\"stop and frisks 2016\",\n",
    "       xlabel=\"X coordinates\",\n",
    "       ylabel=\"Y coordinates\") \n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### its alive!  bring in some other data to see if everything appears to be in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import datasets\n",
    "pr = gpd.read_file('./data/nypd-precincts.geojson') # import precincts geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first double-check the CRS on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check CRS\n",
    "pr.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize these new layers\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "base_color = 'white'\n",
    "border_color = base_color\n",
    "\n",
    "# layers to print\n",
    "gdf.plot(ax=ax, color='red', alpha=0.04, zorder=3, label='stops');  #low opacity adds a heatmap effect at this scale.  unintentional\n",
    "pr.plot(ax=ax, edgecolor='black', color='white', alpha=1, legend = True, zorder=2);\n",
    "\n",
    "# add title and labels\n",
    "ax.set(title=\"stop and frisks by Precinct\",\n",
    "       xlabel=\"X coordinates\",\n",
    "       ylabel=\"Y coordinates\") \n",
    "\n",
    "# reset bounds to match NYC data\n",
    "ax.set(xlim=(-74.3,-73.6), ylim=(40.25,41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### let's merge the precinct \"counts\" into the precincts geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns\n",
    "pr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check data types to make sure they match up\n",
    "type(counts.precinct[0]), type(pr.precinct[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integer to string\n",
    "pr.precinct = pr.precinct.astype(str)\n",
    "\n",
    "# first check data types to make sure they match up\n",
    "type(counts.precinct[0]), type(pr.precinct[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with `merge` method on shared variable (precinct):\n",
    "pr = pr.merge(counts, on='precinct')\n",
    "print(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we can visualize the Precincts geojson, as a chloropeth, with the counts of Stop and Frisk Incidents per precinct as the ramp value. \n",
    "this uses matplotlib's colormap parameter `cmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    " \n",
    "base_color = 'white'\n",
    "border_color = base_color\n",
    "\n",
    "# layers to print\n",
    "pr.plot(ax=ax, edgecolor='black', column='counts', cmap = 'RdPu', alpha=1, legend = True, zorder=2);\n",
    "gdf.plot(ax=ax, markersize=1, alpha=1, color='black', zorder=3);  #low opacity adds a heatmap effect at this scale.\n",
    "\n",
    "# add title and labels\n",
    "ax.set(title=\"stop and frisks by Precinct\",\n",
    "       xlabel=\"X coordinates\",\n",
    "       ylabel=\"Y coordinates\") \n",
    "\n",
    "# reset bounds to match NYC data\n",
    "ax.set(xlim=(-74.3,-73.6), ylim=(40.25,41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quickly take a look at `pr` Precincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking these data inspection commands is possible using commas\n",
    "pr.sort_values(by=['precinct'],inplace = True),pr.head(),type(pr.precinct[0]),pr.precinct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pr.precinct[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the busiest precinct.  If you remember from earlier, that was 106. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip the precinct 106\n",
    "pr106 = pr.loc[pr['precinct'] == '106']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check geometry\n",
    "pr106.geom_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bounding box of precinct 106\n",
    "pr106.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot these using the following technique, creating subplots assigned to the fig and ax variables, \n",
    "# setting a couple default variables for the colors, \n",
    "# and plotting a background using the bounding box and the world countries from Natural Earth.\n",
    "fig, ax = plt.subplots()\n",
    " \n",
    "base_color = '#f0f0f0'\n",
    "border_color = base_color\n",
    " \n",
    "pr106.plot(ax=ax, color='white', edgecolor='black', zorder=0)\n",
    "gdf.plot(ax=ax, color='black', alpha=0.1, zorder=6);\n",
    "\n",
    "\n",
    "ax.set(title=\"precinct 106 Stop and Frisks\",\n",
    "       xlabel=\"Longitude\",\n",
    "       ylabel=\"Latitude\")\n",
    "\n",
    "# reset bounds to match precinct 106 bounds\n",
    "ax.set(xlim=(-73.86,-73.8), ylim=(40.64,40.69))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### export the stop/frisk data as webmap ready GEOJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as geojson \n",
    "### throws an error unless file is deleted prior\n",
    "gdf.to_file(r'./data/2016stops.geojson', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#double check the data, for ASMR purposes alone\n",
    "for root, dirs, files in os.walk('./data/'):\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally! export the notebook as an HTML file for easy webviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\Scripts\\jupyter-nbconvert-script.py\", line 9, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\jupyter_core\\application.py\", line 267, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 338, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 498, in convert_notebooks\n",
      "    self.exporter = cls(config=self.config)\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\nbconvert\\exporters\\templateexporter.py\", line 255, in __init__\n",
      "    super(TemplateExporter, self).__init__(config=config, **kw)\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 110, in __init__\n",
      "    self._init_preprocessors()\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 264, in _init_preprocessors\n",
      "    self.register_preprocessor(preprocessor, enabled=True)\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 225, in register_preprocessor\n",
      "    preprocessor_cls = import_item(preprocessor)\n",
      "  File \"C:\\Users\\rkatko0001\\AppData\\Local\\Continuum\\anaconda3\\envs\\module-06\\lib\\site-packages\\traitlets\\utils\\importstring.py\", line 34, in import_item\n",
      "    module = __import__(package, fromlist=[obj])\n",
      "ModuleNotFoundError: No module named 'jupyter_contrib_nbextensions'\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html rakatk0-python-pandas-geopandas-101.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
